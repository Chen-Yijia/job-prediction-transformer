{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToWord(title):\n",
    "    target_words=[]\n",
    "    for word in title.split(' '):\n",
    "        if word != '' and word != ' ':\n",
    "            target_words.append(word)\n",
    "        \n",
    "    return target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareTrainSet():\n",
    "    with open('Book3.csv','r') as r:\n",
    "        reader=csv.reader(r)\n",
    "        index=0\n",
    "        train_set={}\n",
    "\n",
    "            \n",
    "        for row in reader:\n",
    "            person=[] \n",
    "            title_sequence=[]\n",
    "            #delete the empty element\n",
    "            for element in row:\n",
    "                if len(element) == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    person.append(element) #person: id+job_title_sequence\n",
    "            title_sequence=person[1:]      #title_sequnce: job sequence without id\n",
    "            max_length=len(title_sequence)-1\n",
    "            for i in range(1,max_length+1):\n",
    "                for j in range(0,len(title_sequence)-i):\n",
    "                    data={}\n",
    "                    data['input']=[]\n",
    "                    for k in range(j,i+j):\n",
    "                        data['input'].append(title_sequence[k])\n",
    "#                        data['input'].append(str(EOS_token))\n",
    "                    title=title_sequence[i+j]\n",
    "                    data['target']=ToWord(title)\n",
    "#                    data['target'].append(str(EOS_token))\n",
    "                    train_set[index]=data\n",
    "                    index += 1\n",
    "           \n",
    "    return train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = PrepareTrainSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315174"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenTrainSet=len(train_set)\n",
    "lenTrainSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordbank & Titlebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word():\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addTrainSet(self, train_set):\n",
    "#        for word in sentence.split(' '):\n",
    "#            self.addWord(word)\n",
    "        for i in range(len(train_set)):\n",
    "            for word in train_set[i]['target']:\n",
    "                self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Title():\n",
    "    def __init__(self):\n",
    "        self.title2index = {}\n",
    "        self.title2count = {}\n",
    "        self.index2title = {}\n",
    "        self.n_titles = 0  # Count SOS and EOS\n",
    "\n",
    "    def addTrainSet(self, train_set):\n",
    "#        for word in sentence.split(' '):\n",
    "#            self.addWord(word)\n",
    "        for i in range(len(train_set)):\n",
    "            for title in train_set[i]['input']:\n",
    "                self.addTitle(title)\n",
    "\n",
    "    def addTitle(self, title):\n",
    "        if title not in self.title2index:\n",
    "            self.title2index[title] = self.n_titles\n",
    "            self.title2count[title] = 1\n",
    "            self.index2title[self.n_titles] = title\n",
    "            self.n_titles += 1\n",
    "        else:\n",
    "            self.title2count[title] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordbank=Word()\n",
    "wordbank.addTrainSet(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "titlebank=Title()\n",
    "titlebank.addTrainSet(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,train_set):\n",
    "        self.train_set=train_set\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_set)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        input_indexes=[]\n",
    "        target_indexes=[SOS_token]\n",
    "        \n",
    "        for item in self.train_set[index]['input']:\n",
    "            input_indexes.append(titlebank.title2index[item])\n",
    "        input_tensor = torch.tensor(input_indexes, dtype = torch.long, device = device).view(-1,1)\n",
    "        \n",
    "        for word in self.train_set[index]['target']:\n",
    "            target_indexes.append(wordbank.word2index[word])\n",
    "        target_indexes.append(EOS_token)\n",
    "        target_tensor = torch.tensor(target_indexes, dtype = torch.long, device = device).view(-1,1)\n",
    "        \n",
    "        sample={'input':input_tensor, 'target':target_tensor}\n",
    "        # input tensor: tensor(index1,index2...)\n",
    "      \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d_model = ninp (embedding dimension) <br>\n",
    "nhid: the dimension of the feedforward network model in nn.TransformerEncoder <br>\n",
    "nhead: the number of heads in the multiheadattention models  <br>\n",
    "nlayers: the number of nn.TransformerEncoderLayer in nn.TransformerEncoder  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### questions: \n",
    "1. why the decoder is ntoken? so the input and output tokens are the same?\n",
    "2. is ntokens here means the number of the words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_titles, n_words, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoderLayer, TransformerDecoder\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.trg_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout) #get the positional encoder\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout) #get a layer\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers) # get the encoder\n",
    "        self.encoder = nn.Embedding(n_titles, ninp) #embedding layer\n",
    "        \n",
    "        self.decoder = nn.Embedding(n_words, ninp) #decoder embedding\n",
    "        decoder_layers = TransformerDecoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layers, nlayers)\n",
    "        self.ninp = ninp\n",
    "        self.out = nn.Linear(ninp, n_words)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out.bias.data.zero_()\n",
    "        self.out.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            src_device = src.device\n",
    "            src_mask = self._generate_square_subsequent_mask(len(src)).to(src_device)\n",
    "            self.src_mask = src_mask\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n",
    "            trg_device = trg.device\n",
    "            trg_mask = self._generate_square_subsequent_mask(len(trg)).to(trg_device)\n",
    "            self.trg_mask = trg_mask\n",
    "            \n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        enc_output = self.transformer_encoder(src, self.src_mask)\n",
    "        trg = self.decoder(trg) * math.sqrt(self.ninp)\n",
    "        trg = self.pos_encoder(trg)\n",
    "        dec_output = self.transformer_decoder(trg, enc_output, self.trg_mask)\n",
    "        preds = self.out(dec_output)\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_titles = titlebank.n_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48949"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = wordbank.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20187"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate an instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_titles = titlebank.n_titles\n",
    "n_words = wordbank.n_words\n",
    "ninp = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 6 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "\n",
    "model = TransformerModel(n_titles, n_words, ninp, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseFromDataset(index):\n",
    "    sample=positionsDataset[index]\n",
    "    return (sample['input'],sample['target'])\n",
    "\n",
    "positionsDataset = PositionDataset(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 5000\n",
    "n_evaluate = 2000\n",
    "n_test = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pairs = [chooseFromDataset(random.randint(0,lenTrainSet-1)) for i in range(n_iters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluating_pairs = [chooseFromDataset(random.randint(0,lenTrainSet-1)) for i in range(n_evaluate)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train( ) and evaluate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.0005 # learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "outputs: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),torch.Size([21])\n",
      "preds: torch.Size([5, 1, 20187])\n",
      "preds_flat: torch.Size([5, 20187])\n",
      "target_out: torch.Size([5, 1])\n",
      "shaped: torch.Size([5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For test\n",
    "n_words = wordbank.n_words\n",
    "training_pair = training_pairs[0]\n",
    "input_tensor = training_pair[0]\n",
    "target_tensor = training_pair[1]\n",
    "trg_input = target_tensor[:-1,:]\n",
    "trg_output = target_tensor[1:,:]\n",
    "\n",
    "outputs = torch.zeros(21).type_as(input_tensor.data)\n",
    "preds = model(input_tensor, trg_input)\n",
    "preds_flat = preds.view(-1,n_words)\n",
    "shaped = trg_output.reshape(-1)\n",
    "print(f'''\n",
    "outputs: {outputs},{outputs.shape}\n",
    "preds: {preds.shape}\n",
    "preds_flat: {preds_flat.shape}\n",
    "target_out: {trg_output.shape}\n",
    "shaped: {shaped.shape}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    n_words = wordbank.n_words\n",
    "    \n",
    "    for iter_time in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter_time - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        trg_input = target_tensor[:-1,:]\n",
    "        trg_output = target_tensor[1:,:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(input_tensor, trg_input)\n",
    "\n",
    "        loss = criterion(preds.view(-1,n_words), trg_output.reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if iter_time % log_interval == 0 and iter_time > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} iters | '\n",
    "                  'lr {:02.2f} | ms/iter {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, iter_time, n_iters, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_model, evaluating_pairs):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    n_words = wordbank.n_words\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for iter_time in range(1, n_iters + 1):\n",
    "            training_pair = training_pairs[iter_time - 1]\n",
    "            input_tensor = training_pair[0]\n",
    "            target_tensor = training_pair[1]\n",
    "            trg_input = target_tensor[:-1,:]\n",
    "            trg_output = target_tensor[1:,:]\n",
    "\n",
    "            preds = eval_model(input_tensor, trg_input)\n",
    "            preds_flat = preds.view(-1,n_words)\n",
    "            total_loss += criterion(preds_flat, trg_output.reshape(-1)).item()\n",
    "    return total_loss / len(evaluating_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2000 iters | lr 0.00 | ms/iter 213.77 | loss  4.77 | ppl   118.33\n",
      "| epoch   1 |   400/ 2000 iters | lr 0.00 | ms/iter 196.02 | loss  4.70 | ppl   109.66\n",
      "| epoch   1 |   600/ 2000 iters | lr 0.00 | ms/iter 192.67 | loss  4.55 | ppl    95.07\n",
      "| epoch   1 |   800/ 2000 iters | lr 0.00 | ms/iter 192.35 | loss  4.73 | ppl   113.27\n",
      "| epoch   1 |  1000/ 2000 iters | lr 0.00 | ms/iter 192.54 | loss  4.69 | ppl   109.09\n",
      "| epoch   1 |  1200/ 2000 iters | lr 0.00 | ms/iter 192.23 | loss  4.72 | ppl   112.26\n",
      "| epoch   1 |  1400/ 2000 iters | lr 0.00 | ms/iter 193.30 | loss  4.70 | ppl   109.82\n",
      "| epoch   1 |  1600/ 2000 iters | lr 0.00 | ms/iter 193.34 | loss  4.56 | ppl    95.93\n",
      "| epoch   1 |  1800/ 2000 iters | lr 0.00 | ms/iter 197.64 | loss  4.67 | ppl   106.29\n",
      "| epoch   1 |  2000/ 2000 iters | lr 0.00 | ms/iter 194.49 | loss  4.96 | ppl   143.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 398.74s | valid loss  4.96 | valid ppl   142.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2000 iters | lr 0.00 | ms/iter 207.72 | loss  4.74 | ppl   113.91\n",
      "| epoch   2 |   400/ 2000 iters | lr 0.00 | ms/iter 196.34 | loss  4.66 | ppl   105.50\n",
      "| epoch   2 |   600/ 2000 iters | lr 0.00 | ms/iter 195.73 | loss  4.53 | ppl    92.33\n",
      "| epoch   2 |   800/ 2000 iters | lr 0.00 | ms/iter 206.12 | loss  4.71 | ppl   110.90\n",
      "| epoch   2 |  1000/ 2000 iters | lr 0.00 | ms/iter 203.48 | loss  4.67 | ppl   106.81\n",
      "| epoch   2 |  1200/ 2000 iters | lr 0.00 | ms/iter 200.35 | loss  4.68 | ppl   108.07\n",
      "| epoch   2 |  1400/ 2000 iters | lr 0.00 | ms/iter 199.49 | loss  4.66 | ppl   105.31\n",
      "| epoch   2 |  1600/ 2000 iters | lr 0.00 | ms/iter 199.05 | loss  4.55 | ppl    94.74\n",
      "| epoch   2 |  1800/ 2000 iters | lr 0.00 | ms/iter 198.71 | loss  4.62 | ppl   101.56\n",
      "| epoch   2 |  2000/ 2000 iters | lr 0.00 | ms/iter 198.92 | loss  4.93 | ppl   138.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 409.59s | valid loss  4.98 | valid ppl   145.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2000 iters | lr 0.00 | ms/iter 202.74 | loss  4.69 | ppl   108.64\n",
      "| epoch   3 |   400/ 2000 iters | lr 0.00 | ms/iter 200.79 | loss  4.64 | ppl   103.41\n",
      "| epoch   3 |   600/ 2000 iters | lr 0.00 | ms/iter 202.44 | loss  4.53 | ppl    93.20\n",
      "| epoch   3 |   800/ 2000 iters | lr 0.00 | ms/iter 206.45 | loss  4.69 | ppl   109.02\n",
      "| epoch   3 |  1000/ 2000 iters | lr 0.00 | ms/iter 207.59 | loss  4.65 | ppl   104.73\n",
      "| epoch   3 |  1200/ 2000 iters | lr 0.00 | ms/iter 203.95 | loss  4.67 | ppl   106.51\n",
      "| epoch   3 |  1400/ 2000 iters | lr 0.00 | ms/iter 206.27 | loss  4.65 | ppl   104.10\n",
      "| epoch   3 |  1600/ 2000 iters | lr 0.00 | ms/iter 205.28 | loss  4.55 | ppl    94.87\n",
      "| epoch   3 |  1800/ 2000 iters | lr 0.00 | ms/iter 205.75 | loss  4.62 | ppl   101.28\n",
      "| epoch   3 |  2000/ 2000 iters | lr 0.00 | ms/iter 209.34 | loss  4.90 | ppl   134.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 417.73s | valid loss  5.00 | valid ppl   148.54\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 3 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, evaluating_pairs)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_pairs = [chooseFromDataset(random.randint(0,lenTrainSet-1)) for i in range(n_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.00 | test ppl   148.54\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, testing_pairs)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_print = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateFromDataset(index):\n",
    "    each_set=train_set[index]\n",
    "    sample=positionsDataset[index]\n",
    "    return (each_set['input'],each_set['target'],sample['input'],sample['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "printing_pairs=[evaluateFromDataset(random.randint(0,lenTrainSet-1)) for i in range(n_print)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTitle(best_model, input_tensor, max_len = 21):\n",
    "    best_model.eval()\n",
    "    n_titles = titlebank.n_titles\n",
    "    n_words = wordbank.n_words\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        src_mask = best_model._generate_square_subsequent_mask(len(input_tensor))\n",
    "        src = best_model.pos_encoder(best_model.encoder(input_tensor)*math.sqrt(best_model.ninp))\n",
    "        enc_output = best_model.transformer_encoder(src,src_mask)\n",
    "        \n",
    "        outputs = torch.zeros(max_len).type_as(input_tensor.data)\n",
    "        outputs[0] = torch.LongTensor([SOS_token])\n",
    "#         print('enc:', enc_output)\n",
    "        for i in range(1, max_len):\n",
    "            trg_mask = best_model._generate_square_subsequent_mask(i)\n",
    "            trg = best_model.pos_encoder(best_model.decoder(outputs[:i].unsqueeze(1))*math.sqrt(best_model.ninp))\n",
    "            print('trg:',trg)\n",
    "            dec_output = best_model.transformer_decoder(trg, enc_output, trg_mask)\n",
    "#             print('dec_out:', dec_output)\n",
    "            dec_output = best_model.out(dec_output)\n",
    "            out_flat = dec_output.view(-1,n_words)\n",
    "            final = F.log_softmax(out_flat, dim=1)\n",
    "\n",
    "            topv, topi = final[-1,:].data.topk(1)\n",
    "            outputs[i] = topi.item()\n",
    "            if topi.item() == EOS_token:\n",
    "                break\n",
    "                \n",
    "    return ' '.join(wordbank.index2word[idx] for idx in outputs[:i].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(best_model, n=10):\n",
    "    for i in range(n):\n",
    "        pair = printing_pairs[i]\n",
    "        print('input sequence: ', pair[0])\n",
    "        target_title = ' '.join(pair[1])\n",
    "        print('target title: ', target_title)\n",
    "        output_title = generateTitle(best_model, pair[2])\n",
    "        print('output title: ', output_title)\n",
    "        print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sequence:  ['Trader', 'Product manager', 'Fund manager Emerging Markets', 'Head of Asset Allocation']\n",
      "target title:  Founding Partner - CEO\n",
      "output title:  SOS\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Associate']\n",
      "target title:  Executive Director\n",
      "output title:  SOS\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Director - eCommerce', 'Expat country executive - Global managed services & Captive Operations setup', 'AVP - Operational excellence and transformations', 'Regional M&A Lead - Middle East, Africa and South Asia', 'India head - ITO, sourcing and delivery excellence', 'Americas Head - Transformations and effectiveness']\n",
      "target title:  Founder and CEO\n",
      "output title:  SOS\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Project Manager', 'Managing Director']\n",
      "target title:  Area Director\n",
      "output title:  SOS\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Banquet Server', 'Business Analyst']\n",
      "target title:  Guest Relations Officer\n",
      "output title:  SOS\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Art Director']\n",
      "target title:  Senior Art Director\n",
      "output title:  SOS\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Pharmacist', 'Manager, Marketing & Regulatory Affairs']\n",
      "target title:  Manager, Regulatory Affairs & Quality Assurance ASEAN\n",
      "output title:  SOS\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Senior Programmer', 'Systems Engineer', 'Senior Manager', 'Staff Consultant', 'VP']\n",
      "target title:  Director\n",
      "output title:  SOS\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Construction Supervisor']\n",
      "target title:  Welding Superintendent/Norce Endeavour\n",
      "output title:  SOS\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Civil Servant', 'Manager, Admin & HR', 'Assistant Manager HR']\n",
      "target title:  Assistant Human Capital Development Manager\n",
      "output title:  SOS\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 5, 1, 1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positionsDataset[3]['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
