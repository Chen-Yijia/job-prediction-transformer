{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToWord(title):\n",
    "    target_words=[]\n",
    "    for word in title.split(' '):\n",
    "        if word != '' and word != ' ':\n",
    "            target_words.append(word)\n",
    "        \n",
    "    return target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareTrainSet():\n",
    "    with open('Book3.csv','r') as r:\n",
    "        reader=csv.reader(r)\n",
    "        index=0\n",
    "        train_set={}\n",
    "\n",
    "            \n",
    "        for row in reader:\n",
    "            person=[] \n",
    "            title_sequence=[]\n",
    "            #delete the empty element\n",
    "            for element in row:\n",
    "                if len(element) == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    person.append(element) #person: id+job_title_sequence\n",
    "            title_sequence=person[1:]      #title_sequnce: job sequence without id\n",
    "            max_length=len(title_sequence)-1\n",
    "            for i in range(1,max_length+1):\n",
    "                for j in range(0,len(title_sequence)-i):\n",
    "                    data={}\n",
    "                    data['input']=[]\n",
    "                    for k in range(j,i+j):\n",
    "                        data['input'].append(title_sequence[k])\n",
    "#                        data['input'].append(str(EOS_token))\n",
    "                    title=title_sequence[i+j]\n",
    "                    data['target']=ToWord(title)\n",
    "#                    data['target'].append(str(EOS_token))\n",
    "                    train_set[index]=data\n",
    "                    index += 1\n",
    "           \n",
    "    return train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = PrepareTrainSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315174"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenTrainSet=len(train_set)\n",
    "lenTrainSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterTrainSet(train_set):\n",
    "    final_set = {}\n",
    "    final_idx = 0\n",
    "    \n",
    "    for i in range(len(train_set)):\n",
    "        data = {}\n",
    "        if len(train_set[i]['input']) < len(train_set[i]['target']):\n",
    "            continue\n",
    "        else:\n",
    "            data = train_set[i]\n",
    "            final_set[final_idx] = data\n",
    "            final_idx += 1\n",
    "            \n",
    "    return final_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_set = filterTrainSet(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151228"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenTrainSet=len(final_set)\n",
    "lenTrainSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordbank & Titlebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word():\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addTrainSet(self, train_set):\n",
    "#        for word in sentence.split(' '):\n",
    "#            self.addWord(word)\n",
    "        for i in range(len(train_set)):\n",
    "            for word in train_set[i]['target']:\n",
    "                self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Title():\n",
    "    def __init__(self):\n",
    "        self.title2index = {}\n",
    "        self.title2count = {}\n",
    "        self.index2title = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_titles = 2  # Count SOS and EOS\n",
    "\n",
    "    def addTrainSet(self, train_set):\n",
    "#        for word in sentence.split(' '):\n",
    "#            self.addWord(word)\n",
    "        for i in range(len(train_set)):\n",
    "            for title in train_set[i]['input']:\n",
    "                self.addTitle(title)\n",
    "\n",
    "    def addTitle(self, title):\n",
    "        if title not in self.title2index:\n",
    "            self.title2index[title] = self.n_titles\n",
    "            self.title2count[title] = 1\n",
    "            self.index2title[self.n_titles] = title\n",
    "            self.n_titles += 1\n",
    "        else:\n",
    "            self.title2count[title] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordbank=Word()\n",
    "wordbank.addTrainSet(final_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "titlebank=Title()\n",
    "titlebank.addTrainSet(final_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,train_set):\n",
    "        self.train_set=train_set\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_set)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        input_indexes=[]\n",
    "        target_indexes=[]\n",
    "        \n",
    "        for item in self.train_set[index]['input']:\n",
    "            input_indexes.append(titlebank.title2index[item])\n",
    "        input_indexes.append(EOS_token)\n",
    "        input_tensor = torch.tensor(input_indexes, dtype = torch.long, device = device).view(-1,1)\n",
    "        \n",
    "        for word in self.train_set[index]['target']:\n",
    "            target_indexes.append(wordbank.word2index[word])\n",
    "        target_indexes.append(EOS_token)\n",
    "        while len(target_indexes) < len(input_indexes):\n",
    "            target_indexes.append(EOS_token)\n",
    "        target_tensor = torch.tensor(target_indexes, dtype = torch.long, device = device).view(-1)\n",
    "        \n",
    "        sample={'input':input_tensor, 'target':target_tensor}\n",
    "        # input tensor: tensor(index1,index2...)\n",
    "      \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d_model = ninp (embedding dimension) <br>\n",
    "nhid: the dimension of the feedforward network model in nn.TransformerEncoder <br>\n",
    "nhead: the number of heads in the multiheadattention models  <br>\n",
    "nlayers: the number of nn.TransformerEncoderLayer in nn.TransformerEncoder  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### questions: \n",
    "1. why the decoder is ntoken? so the input and output tokens are the same?\n",
    "2. is ntokens here means the number of the words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_titles, n_words, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout) #get the positional encoder\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout) #get a layer\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers) # get the encoder\n",
    "        self.encoder = nn.Embedding(n_titles, ninp) #embedding layer\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, n_words)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_titles = titlebank.n_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42302"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = wordbank.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9186"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate an instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_titles = titlebank.n_titles\n",
    "n_words = wordbank.n_words\n",
    "ninp = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "\n",
    "model = TransformerModel(n_titles, n_words, ninp, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseFromDataset(index):\n",
    "    sample=positionsDataset[index]\n",
    "    return (sample['input'],sample['target'])\n",
    "\n",
    "positionsDataset = PositionDataset(final_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pairs = [chooseFromDataset(random.randint(0,lenTrainSet-1)) for i in range(n_iters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluating_pairs = [chooseFromDataset(random.randint(0,lenTrainSet-1)) for i in range(n_iters)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train( ) and evaluate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train():\n",
    "\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    n_words = wordbank.n_words\n",
    "\n",
    "    for iter_time in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter_time - 1]  \n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        seq_length = len(input_tensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_tensor)\n",
    "        output_flat = output.view(-1,n_words)\n",
    "        \n",
    "        loss = 0.\n",
    "        for i in range(seq_length):\n",
    "            loss_element = criterion(output_flat[i].view(-1,n_words),target_tensor[i].view(-1))\n",
    "            loss += loss_element\n",
    "            \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()/seq_length\n",
    "        log_interval = 200\n",
    "        if iter_time % log_interval == 0 and iter_time > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} iters | '\n",
    "                  'lr {:02.2f} | ms/iter {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, iter_time, n_iters, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_model, evaluating_pairs):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    n_words = wordbank.n_words\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for iter_time in range(1, n_iters + 1):\n",
    "            training_pair = training_pairs[iter_time - 1]\n",
    "            input_tensor = training_pair[0]\n",
    "            target_tensor = training_pair[1]\n",
    "            seq_length = len(input_tensor)\n",
    "        \n",
    "            output = eval_model(input_tensor)\n",
    "            output_flat = output.view(-1,n_words)\n",
    "            \n",
    "            loss = 0.\n",
    "            for i in range(seq_length):\n",
    "                loss_element = criterion(output_flat[i].view(-1,n_words),target_tensor[i].view(-1))\n",
    "                loss += loss_element\n",
    "            total_loss += loss.item()/seq_length\n",
    "    return total_loss / len(evaluating_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 3 # The number of epochs\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2000 iters | lr 5.00 | ms/iter 63.72 | loss  4.50 | ppl    90.00\n",
      "| epoch   1 |   400/ 2000 iters | lr 5.00 | ms/iter 64.18 | loss  4.54 | ppl    93.49\n",
      "| epoch   1 |   600/ 2000 iters | lr 5.00 | ms/iter 72.81 | loss  4.44 | ppl    84.51\n",
      "| epoch   1 |   800/ 2000 iters | lr 5.00 | ms/iter 64.19 | loss  4.60 | ppl    99.66\n",
      "| epoch   1 |  1000/ 2000 iters | lr 5.00 | ms/iter 61.59 | loss  4.60 | ppl    99.58\n",
      "| epoch   1 |  1200/ 2000 iters | lr 5.00 | ms/iter 61.11 | loss  4.54 | ppl    93.58\n",
      "| epoch   1 |  1400/ 2000 iters | lr 5.00 | ms/iter 62.40 | loss  4.51 | ppl    90.69\n",
      "| epoch   1 |  1600/ 2000 iters | lr 5.00 | ms/iter 62.44 | loss  4.35 | ppl    77.58\n",
      "| epoch   1 |  1800/ 2000 iters | lr 5.00 | ms/iter 64.38 | loss  4.47 | ppl    87.47\n",
      "| epoch   1 |  2000/ 2000 iters | lr 5.00 | ms/iter 62.57 | loss  4.36 | ppl    77.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 131.23s | valid loss  5.12 | valid ppl   168.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2000 iters | lr 4.51 | ms/iter 61.28 | loss  4.25 | ppl    70.18\n",
      "| epoch   2 |   400/ 2000 iters | lr 4.51 | ms/iter 62.10 | loss  4.38 | ppl    79.55\n",
      "| epoch   2 |   600/ 2000 iters | lr 4.51 | ms/iter 65.43 | loss  4.25 | ppl    70.10\n",
      "| epoch   2 |   800/ 2000 iters | lr 4.51 | ms/iter 67.75 | loss  4.41 | ppl    81.88\n",
      "| epoch   2 |  1000/ 2000 iters | lr 4.51 | ms/iter 65.26 | loss  4.41 | ppl    82.56\n",
      "| epoch   2 |  1200/ 2000 iters | lr 4.51 | ms/iter 63.72 | loss  4.35 | ppl    77.67\n",
      "| epoch   2 |  1400/ 2000 iters | lr 4.51 | ms/iter 63.71 | loss  4.26 | ppl    71.01\n",
      "| epoch   2 |  1600/ 2000 iters | lr 4.51 | ms/iter 72.51 | loss  4.14 | ppl    62.50\n",
      "| epoch   2 |  1800/ 2000 iters | lr 4.51 | ms/iter 65.46 | loss  4.30 | ppl    73.39\n",
      "| epoch   2 |  2000/ 2000 iters | lr 4.51 | ms/iter 63.47 | loss  4.29 | ppl    72.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 133.60s | valid loss  4.96 | valid ppl   143.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2000 iters | lr 4.29 | ms/iter 61.98 | loss  4.11 | ppl    60.74\n",
      "| epoch   3 |   400/ 2000 iters | lr 4.29 | ms/iter 62.81 | loss  4.33 | ppl    75.76\n",
      "| epoch   3 |   600/ 2000 iters | lr 4.29 | ms/iter 62.76 | loss  4.17 | ppl    64.68\n",
      "| epoch   3 |   800/ 2000 iters | lr 4.29 | ms/iter 62.30 | loss  4.26 | ppl    70.93\n",
      "| epoch   3 |  1000/ 2000 iters | lr 4.29 | ms/iter 65.61 | loss  4.22 | ppl    67.89\n",
      "| epoch   3 |  1200/ 2000 iters | lr 4.29 | ms/iter 64.92 | loss  4.17 | ppl    64.93\n",
      "| epoch   3 |  1400/ 2000 iters | lr 4.29 | ms/iter 66.13 | loss  4.12 | ppl    61.67\n",
      "| epoch   3 |  1600/ 2000 iters | lr 4.29 | ms/iter 63.16 | loss  3.96 | ppl    52.36\n",
      "| epoch   3 |  1800/ 2000 iters | lr 4.29 | ms/iter 62.58 | loss  4.11 | ppl    60.76\n",
      "| epoch   3 |  2000/ 2000 iters | lr 4.29 | ms/iter 64.27 | loss  4.14 | ppl    62.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 130.71s | valid loss  4.69 | valid ppl   108.36\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, evaluating_pairs)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_pairs = [chooseFromDataset(random.randint(0,lenTrainSet-1)) for i in range(n_iters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  4.69 | test ppl   108.36\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, testing_pairs)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_print = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateFromDataset(index):\n",
    "    each_set=final_set[index]\n",
    "    sample=positionsDataset[index]\n",
    "    return (each_set['input'],each_set['target'],sample['input'],sample['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "printing_pairs=[evaluateFromDataset(random.randint(0,lenTrainSet-1)) for i in range(n_print)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalPrint(eval_model, input_tensor):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    n_words = wordbank.n_words\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        output = eval_model(input_tensor)\n",
    "        output_flat = output.view(-1,n_words)\n",
    "        softmax = nn.LogSoftmax(dim=1) \n",
    "        final = softmax(output_flat)\n",
    "        decoded_words = []\n",
    "        for i in range(final.size(0)):\n",
    "            topv, topi = final[i].data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(wordbank.index2word[topi.item()])\n",
    "                \n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(model, n=10):\n",
    "    for i in range(n):\n",
    "        pair = printing_pairs[i]\n",
    "        print('input sequence: ', pair[0])\n",
    "        target_title = ' '.join(pair[1])\n",
    "        print('target title: ', target_title)\n",
    "        output_words = evalPrint(model, pair[2])\n",
    "        output_title = ' '.join(output_words)\n",
    "        print('output title: ', output_title)\n",
    "        print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sequence:  ['Executive, Corporate Affairs', 'Senior Marketing Executive, SG Operations Marketing Division', 'Team Leader, SG Fashion Promotion & Marketing']\n",
      "target title:  Campaign Specialist\n",
      "output title:  Director Director Director <EOS>\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Reporter @ Streats', 'Foreign Desk Journalist']\n",
      "target title:  Sub-Editor\n",
      "output title:  Director Director <EOS>\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Director']\n",
      "target title:  Director\n",
      "output title:  Director <EOS>\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Risk Manager and Internal Audit', 'AVP Business Control and Governance']\n",
      "target title:  Vice President\n",
      "output title:  Director Director <EOS>\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Executive Director', 'Group Executive Advisor', 'Project Manager - Compliance', 'Compliance Manager', 'Compliance Manager']\n",
      "target title:  Director\n",
      "output title:  Director Director Director Director Director <EOS>\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Project Coordinator', 'Chief Project Officer', 'Project Manager', 'Project Manager', 'IT Manager']\n",
      "target title:  Solutions Architect\n",
      "output title:  <EOS>\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Warehouse Supervisor', 'Inventory Controller', 'Assistant Warehouse Manager']\n",
      "target title:  DC Operations Manager\n",
      "output title:  <EOS>\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Sales Area Manager', 'Sales Regional Manager (Centre France then North France)', 'Zone Director', 'International Customer Director', 'Key Account & Trade Development Director', 'Trade Development Director', 'Sales Director (Acting)', 'Sales Director']\n",
      "target title:  Director, Head of Key Account Strategy\n",
      "output title:  Director Director Director Director Director Director Director Director <EOS>\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['RSM', 'Business Manager', 'Manager Sales', 'Regional manager', 'Regional Manager', 'Business Development', 'DGM - Sales (Software Services)', 'Sales']\n",
      "target title:  Sales\n",
      "output title:  Director Director Director Director Director Director Director Director <EOS>\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Senior Account Executive', 'Retail Manager', 'Store Manager']\n",
      "target title:  Sales Manager Department\n",
      "output title:  Director Director Director <EOS>\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 5, 1, 1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positionsDataset[3]['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
