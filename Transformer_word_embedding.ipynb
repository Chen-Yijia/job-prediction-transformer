{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToWord(title):\n",
    "    target_words=[]\n",
    "    for word in title.split(' '):\n",
    "        if word != '' and word != ' ':\n",
    "            target_words.append(word)\n",
    "        \n",
    "    return target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareTrainSet():\n",
    "    with open('Book3.csv','r') as r:\n",
    "        reader=csv.reader(r)\n",
    "        index=0\n",
    "        train_set={}\n",
    "\n",
    "            \n",
    "        for row in reader:\n",
    "            person=[] \n",
    "            title_sequence=[]\n",
    "            #delete the empty element\n",
    "            for element in row:\n",
    "                if len(element) == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    person.append(element) #person: id+job_title_sequence\n",
    "            title_sequence=person[1:]      #title_sequnce: job sequence without id\n",
    "            max_length=len(title_sequence)-1\n",
    "            for i in range(1,max_length+1):\n",
    "                for j in range(0,len(title_sequence)-i):\n",
    "                    data={}\n",
    "                    data['input']=[]\n",
    "                    for k in range(j,i+j):\n",
    "                        data['input'].append(title_sequence[k])\n",
    "#                        data['input'].append(str(EOS_token))\n",
    "                    title=title_sequence[i+j]\n",
    "                    data['target']=ToWord(title)\n",
    "#                    data['target'].append(str(EOS_token))\n",
    "                    train_set[index]=data\n",
    "                    index += 1\n",
    "           \n",
    "    return train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = PrepareTrainSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315174"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenTrainSet=len(train_set)\n",
    "lenTrainSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word():\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addTrainSet(self, train_set):\n",
    "#        for word in sentence.split(' '):\n",
    "#            self.addWord(word)\n",
    "        for i in range(len(train_set)):\n",
    "            for word in train_set[i]['target']:\n",
    "                self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordbank=Word()\n",
    "wordbank.addTrainSet(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings\n",
    "from flair.data import Sentence\n",
    "import flair\n",
    "\n",
    "flair.device = device\n",
    "flair.cache_root='./cache'\n",
    "\n",
    "# initialize the word embeddings\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "\n",
    "# initialize the document embeddings, mode = mean\n",
    "doc_embedding = DocumentPoolEmbeddings([glove_embedding])    # embedding size: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordEmbedding(title_list):\n",
    "    for i in range(len(title_list)):\n",
    "        if i == 0:\n",
    "            try:\n",
    "                sentence=Sentence(title_list[i])\n",
    "                doc_embedding.embed(sentence)\n",
    "                previous_tensor=sentence.embedding.view(1,1,-1)\n",
    "            except:\n",
    "                sentence = Sentence('unknown')\n",
    "                doc_embedding.embed(sentence)\n",
    "                previous_tensor=sentence.embedding.view(1,1,-1)\n",
    "        else:\n",
    "            try:\n",
    "                sentence=Sentence(title)\n",
    "                doc_embedding.embed(sentence)\n",
    "                title_tensor=sentence.embedding.view(1,1,-1)\n",
    "            except:\n",
    "                sentence = Sentence('unknown')\n",
    "                doc_embedding.embed(sentence)\n",
    "                title_tensor=sentence.embedding.view(1,1,-1)\n",
    "                \n",
    "            previous_tensor = torch.cat((previous_tensor,title_tensor),0)\n",
    "            \n",
    "    return previous_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,train_set):\n",
    "        self.train_set=train_set\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_set)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        target_indexes=[SOS_token]\n",
    "        \n",
    "        input_tensor = wordEmbedding(self.train_set[index]['input'])\n",
    "        \n",
    "        for word in self.train_set[index]['target']:\n",
    "            target_indexes.append(wordbank.word2index[word])\n",
    "        target_indexes.append(EOS_token)\n",
    "        target_tensor = torch.tensor(target_indexes, dtype = torch.long, device = device).view(-1,1)\n",
    "        \n",
    "        sample={'input':input_tensor, 'target':target_tensor}\n",
    "        # input tensor: tensor(index1,index2...)\n",
    "      \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "positionsDataset = PositionDataset(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d_model = ninp (embedding dimension) <br>\n",
    "nhid: the dimension of the feedforward network model in nn.TransformerEncoder <br>\n",
    "nhead: the number of heads in the multiheadattention models  <br>\n",
    "nlayers: the number of nn.TransformerEncoderLayer in nn.TransformerEncoder  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=200):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### questions: \n",
    "1. why the decoder is ntoken? so the input and output tokens are the same?\n",
    "2. is ntokens here means the number of the words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_words, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoderLayer, TransformerDecoder\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.trg_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout) #get the positional encoder\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout) #get a layer\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers) # get the encoder\n",
    "        \n",
    "        self.decoder = nn.Embedding(n_words, ninp) #decoder embedding\n",
    "        decoder_layers = TransformerDecoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layers, nlayers)\n",
    "        self.ninp = ninp\n",
    "        self.out = nn.Linear(ninp, n_words)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.out.bias.data.zero_()\n",
    "        self.out.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            src_device = src.device\n",
    "            src_mask = self._generate_square_subsequent_mask(len(src)).to(src_device)\n",
    "            self.src_mask = src_mask\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n",
    "            trg_device = trg.device\n",
    "            trg_mask = self._generate_square_subsequent_mask(len(trg)).to(trg_device)\n",
    "            self.trg_mask = trg_mask\n",
    "            \n",
    "        src = src * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        enc_output = self.transformer_encoder(src, self.src_mask)\n",
    "        trg = self.decoder(trg) * math.sqrt(self.ninp)\n",
    "        trg = self.pos_encoder(trg)\n",
    "        dec_output = self.transformer_decoder(trg, enc_output, self.trg_mask)\n",
    "        preds = self.out(dec_output)\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = wordbank.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20187"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate an instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = wordbank.n_words\n",
    "ninp = 100 # embedding dimension\n",
    "nhid = 100 # the dimension of the feedforward network model in nn.TransformerEncoder & nn.TransformerDecoder\n",
    "nlayers = 6 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 4 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "\n",
    "model = TransformerModel(n_words, ninp, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseFromDataset(index):\n",
    "    sample=positionsDataset[index]\n",
    "    return (sample['input'],sample['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 5000\n",
    "n_evaluate = 2000\n",
    "n_test = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pairs = [chooseFromDataset(random.randint(0,lenTrainSet-1)) for i in range(n_iters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluating_pairs = [chooseFromDataset(random.randint(0,lenTrainSet-1)) for i in range(n_evaluate)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train( ) and evaluate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.0005 # learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "outputs: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),torch.Size([21])\n",
      "preds: torch.Size([3, 1, 20187])\n",
      "preds_flat: torch.Size([3, 20187])\n",
      "target_out: torch.Size([3, 1])\n",
      "shaped: torch.Size([3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For test\n",
    "n_words = wordbank.n_words\n",
    "training_pair = training_pairs[0]\n",
    "input_tensor = training_pair[0]\n",
    "target_tensor = training_pair[1]\n",
    "trg_input = target_tensor[:-1,:]\n",
    "trg_output = target_tensor[1:,:]\n",
    "\n",
    "outputs = torch.zeros(21).type_as(input_tensor.data)\n",
    "preds = model(input_tensor, trg_input)\n",
    "preds_flat = preds.view(-1,n_words)\n",
    "shaped = trg_output.reshape(-1)\n",
    "print(f'''\n",
    "outputs: {outputs},{outputs.shape}\n",
    "preds: {preds.shape}\n",
    "preds_flat: {preds_flat.shape}\n",
    "target_out: {trg_output.shape}\n",
    "shaped: {shaped.shape}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train(epoch):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    n_words = wordbank.n_words\n",
    "    \n",
    "    for iter_time in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter_time - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        trg_input = target_tensor[:-1,:]\n",
    "        trg_output = target_tensor[1:,:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(input_tensor, trg_input)\n",
    "\n",
    "        loss = criterion(preds.view(-1,n_words), trg_output.reshape(-1))\n",
    "        if epoch != epochs:\n",
    "            loss.backward(retain_graph=True)\n",
    "        else:\n",
    "            loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if iter_time % log_interval == 0 and iter_time > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} iters | '\n",
    "                  'lr {:02.2f} | ms/iter {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, iter_time, n_iters, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_model, evaluating_pairs):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    n_words = wordbank.n_words\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for iter_time in range(1, n_iters + 1):\n",
    "            training_pair = training_pairs[iter_time - 1]\n",
    "            input_tensor = training_pair[0]\n",
    "            target_tensor = training_pair[1]\n",
    "            trg_input = target_tensor[:-1,:]\n",
    "            trg_output = target_tensor[1:,:]\n",
    "\n",
    "            preds = eval_model(input_tensor, trg_input)\n",
    "            preds_flat = preds.view(-1,n_words)\n",
    "            total_loss += criterion(preds_flat, trg_output.reshape(-1)).item()\n",
    "    return total_loss / len(evaluating_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(epochs=3):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(epoch)\n",
    "        val_loss = evaluate(model, evaluating_pairs)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                         val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "    \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model\n",
    "    \n",
    "        scheduler.step()\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 5000 iters | lr 0.00 | ms/iter 73.89 | loss  6.85 | ppl   947.82\n",
      "| epoch   1 |   400/ 5000 iters | lr 0.00 | ms/iter 81.09 | loss  5.68 | ppl   292.67\n",
      "| epoch   1 |   600/ 5000 iters | lr 0.00 | ms/iter 79.45 | loss  5.62 | ppl   276.43\n",
      "| epoch   1 |   800/ 5000 iters | lr 0.00 | ms/iter 81.10 | loss  5.57 | ppl   261.17\n",
      "| epoch   1 |  1000/ 5000 iters | lr 0.00 | ms/iter 80.76 | loss  5.48 | ppl   240.93\n",
      "| epoch   1 |  1200/ 5000 iters | lr 0.00 | ms/iter 77.58 | loss  5.45 | ppl   232.11\n",
      "| epoch   1 |  1400/ 5000 iters | lr 0.00 | ms/iter 77.75 | loss  5.43 | ppl   228.55\n",
      "| epoch   1 |  1600/ 5000 iters | lr 0.00 | ms/iter 74.36 | loss  5.23 | ppl   187.50\n",
      "| epoch   1 |  1800/ 5000 iters | lr 0.00 | ms/iter 73.96 | loss  5.54 | ppl   255.21\n",
      "| epoch   1 |  2000/ 5000 iters | lr 0.00 | ms/iter 75.65 | loss  5.58 | ppl   265.56\n",
      "| epoch   1 |  2200/ 5000 iters | lr 0.00 | ms/iter 75.11 | loss  5.48 | ppl   239.33\n",
      "| epoch   1 |  2400/ 5000 iters | lr 0.00 | ms/iter 74.02 | loss  5.49 | ppl   242.09\n",
      "| epoch   1 |  2600/ 5000 iters | lr 0.00 | ms/iter 75.86 | loss  5.61 | ppl   272.00\n",
      "| epoch   1 |  2800/ 5000 iters | lr 0.00 | ms/iter 74.74 | loss  5.63 | ppl   277.89\n",
      "| epoch   1 |  3000/ 5000 iters | lr 0.00 | ms/iter 75.40 | loss  5.48 | ppl   241.00\n",
      "| epoch   1 |  3200/ 5000 iters | lr 0.00 | ms/iter 74.89 | loss  5.31 | ppl   201.36\n",
      "| epoch   1 |  3400/ 5000 iters | lr 0.00 | ms/iter 75.08 | loss  5.48 | ppl   239.03\n",
      "| epoch   1 |  3600/ 5000 iters | lr 0.00 | ms/iter 76.53 | loss  5.36 | ppl   212.65\n",
      "| epoch   1 |  3800/ 5000 iters | lr 0.00 | ms/iter 76.31 | loss  5.36 | ppl   212.87\n",
      "| epoch   1 |  4000/ 5000 iters | lr 0.00 | ms/iter 75.62 | loss  5.24 | ppl   188.21\n",
      "| epoch   1 |  4200/ 5000 iters | lr 0.00 | ms/iter 76.11 | loss  5.57 | ppl   261.47\n",
      "| epoch   1 |  4400/ 5000 iters | lr 0.00 | ms/iter 75.54 | loss  5.73 | ppl   309.28\n",
      "| epoch   1 |  4600/ 5000 iters | lr 0.00 | ms/iter 75.57 | loss  5.24 | ppl   188.51\n",
      "| epoch   1 |  4800/ 5000 iters | lr 0.00 | ms/iter 76.15 | loss  5.44 | ppl   230.47\n",
      "| epoch   1 |  5000/ 5000 iters | lr 0.00 | ms/iter 75.81 | loss  5.55 | ppl   256.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 415.74s | valid loss 12.56 | valid ppl 285384.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 5000 iters | lr 0.00 | ms/iter 76.22 | loss  4.76 | ppl   116.37\n",
      "| epoch   2 |   400/ 5000 iters | lr 0.00 | ms/iter 75.80 | loss  4.81 | ppl   123.16\n",
      "| epoch   2 |   600/ 5000 iters | lr 0.00 | ms/iter 76.18 | loss  4.80 | ppl   121.54\n",
      "| epoch   2 |   800/ 5000 iters | lr 0.00 | ms/iter 81.87 | loss  4.81 | ppl   123.01\n",
      "| epoch   2 |  1000/ 5000 iters | lr 0.00 | ms/iter 78.75 | loss  4.86 | ppl   128.75\n",
      "| epoch   2 |  1200/ 5000 iters | lr 0.00 | ms/iter 85.22 | loss  4.83 | ppl   125.05\n",
      "| epoch   2 |  1400/ 5000 iters | lr 0.00 | ms/iter 89.04 | loss  4.83 | ppl   125.34\n",
      "| epoch   2 |  1600/ 5000 iters | lr 0.00 | ms/iter 86.28 | loss  4.71 | ppl   110.87\n",
      "| epoch   2 |  1800/ 5000 iters | lr 0.00 | ms/iter 85.00 | loss  4.93 | ppl   138.34\n",
      "| epoch   2 |  2000/ 5000 iters | lr 0.00 | ms/iter 99.11 | loss  4.94 | ppl   140.04\n",
      "| epoch   2 |  2200/ 5000 iters | lr 0.00 | ms/iter 147.25 | loss  4.91 | ppl   136.10\n",
      "| epoch   2 |  2400/ 5000 iters | lr 0.00 | ms/iter 100.39 | loss  4.97 | ppl   144.71\n",
      "| epoch   2 |  2600/ 5000 iters | lr 0.00 | ms/iter 78.92 | loss  4.97 | ppl   144.71\n",
      "| epoch   2 |  2800/ 5000 iters | lr 0.00 | ms/iter 76.31 | loss  5.01 | ppl   149.93\n",
      "| epoch   2 |  3000/ 5000 iters | lr 0.00 | ms/iter 76.10 | loss  5.00 | ppl   148.46\n",
      "| epoch   2 |  3200/ 5000 iters | lr 0.00 | ms/iter 76.17 | loss  4.87 | ppl   130.14\n",
      "| epoch   2 |  3400/ 5000 iters | lr 0.00 | ms/iter 76.38 | loss  4.97 | ppl   144.08\n",
      "| epoch   2 |  3600/ 5000 iters | lr 0.00 | ms/iter 76.31 | loss  4.94 | ppl   139.49\n",
      "| epoch   2 |  3800/ 5000 iters | lr 0.00 | ms/iter 76.48 | loss  4.87 | ppl   130.83\n",
      "| epoch   2 |  4000/ 5000 iters | lr 0.00 | ms/iter 76.81 | loss  4.78 | ppl   118.63\n",
      "| epoch   2 |  4200/ 5000 iters | lr 0.00 | ms/iter 80.04 | loss  5.01 | ppl   149.20\n",
      "| epoch   2 |  4400/ 5000 iters | lr 0.00 | ms/iter 80.43 | loss  5.23 | ppl   187.12\n",
      "| epoch   2 |  4600/ 5000 iters | lr 0.00 | ms/iter 80.57 | loss  4.88 | ppl   132.10\n",
      "| epoch   2 |  4800/ 5000 iters | lr 0.00 | ms/iter 79.86 | loss  5.05 | ppl   156.75\n",
      "| epoch   2 |  5000/ 5000 iters | lr 0.00 | ms/iter 100.73 | loss  5.18 | ppl   178.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 462.70s | valid loss 12.52 | valid ppl 273598.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 5000 iters | lr 0.00 | ms/iter 79.95 | loss  4.72 | ppl   112.30\n",
      "| epoch   3 |   400/ 5000 iters | lr 0.00 | ms/iter 77.66 | loss  4.73 | ppl   113.10\n",
      "| epoch   3 |   600/ 5000 iters | lr 0.00 | ms/iter 76.72 | loss  4.75 | ppl   115.87\n",
      "| epoch   3 |   800/ 5000 iters | lr 0.00 | ms/iter 77.04 | loss  4.76 | ppl   117.21\n",
      "| epoch   3 |  1000/ 5000 iters | lr 0.00 | ms/iter 76.90 | loss  4.82 | ppl   124.24\n",
      "| epoch   3 |  1200/ 5000 iters | lr 0.00 | ms/iter 77.32 | loss  4.78 | ppl   119.38\n",
      "| epoch   3 |  1400/ 5000 iters | lr 0.00 | ms/iter 78.91 | loss  4.82 | ppl   123.37\n",
      "| epoch   3 |  1600/ 5000 iters | lr 0.00 | ms/iter 78.68 | loss  4.64 | ppl   103.95\n",
      "| epoch   3 |  1800/ 5000 iters | lr 0.00 | ms/iter 77.95 | loss  4.88 | ppl   131.74\n",
      "| epoch   3 |  2000/ 5000 iters | lr 0.00 | ms/iter 77.38 | loss  4.86 | ppl   129.60\n",
      "| epoch   3 |  2200/ 5000 iters | lr 0.00 | ms/iter 77.27 | loss  4.82 | ppl   124.34\n",
      "| epoch   3 |  2400/ 5000 iters | lr 0.00 | ms/iter 77.25 | loss  4.89 | ppl   133.37\n",
      "| epoch   3 |  2600/ 5000 iters | lr 0.00 | ms/iter 81.23 | loss  4.88 | ppl   131.79\n",
      "| epoch   3 |  2800/ 5000 iters | lr 0.00 | ms/iter 83.21 | loss  4.96 | ppl   142.86\n",
      "| epoch   3 |  3000/ 5000 iters | lr 0.00 | ms/iter 78.71 | loss  4.94 | ppl   140.36\n",
      "| epoch   3 |  3200/ 5000 iters | lr 0.00 | ms/iter 82.29 | loss  4.81 | ppl   122.41\n",
      "| epoch   3 |  3400/ 5000 iters | lr 0.00 | ms/iter 80.12 | loss  4.88 | ppl   131.18\n",
      "| epoch   3 |  3600/ 5000 iters | lr 0.00 | ms/iter 78.71 | loss  4.86 | ppl   128.76\n",
      "| epoch   3 |  3800/ 5000 iters | lr 0.00 | ms/iter 87.75 | loss  4.78 | ppl   118.97\n",
      "| epoch   3 |  4000/ 5000 iters | lr 0.00 | ms/iter 78.49 | loss  4.70 | ppl   110.36\n",
      "| epoch   3 |  4200/ 5000 iters | lr 0.00 | ms/iter 79.09 | loss  4.90 | ppl   134.79\n",
      "| epoch   3 |  4400/ 5000 iters | lr 0.00 | ms/iter 78.98 | loss  5.09 | ppl   162.25\n",
      "| epoch   3 |  4600/ 5000 iters | lr 0.00 | ms/iter 81.44 | loss  4.76 | ppl   116.62\n",
      "| epoch   3 |  4800/ 5000 iters | lr 0.00 | ms/iter 82.46 | loss  4.94 | ppl   139.12\n",
      "| epoch   3 |  5000/ 5000 iters | lr 0.00 | ms/iter 82.13 | loss  5.02 | ppl   151.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 430.32s | valid loss 13.00 | valid ppl 443253.36\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_model = run(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_pairs = [chooseFromDataset(random.randint(0,lenTrainSet-1)) for i in range(n_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 13.00 | test ppl 443253.36\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, testing_pairs)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_print = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateFromDataset(index):\n",
    "    each_set=train_set[index]\n",
    "    sample=positionsDataset[index]\n",
    "    return (each_set['input'],each_set['target'],sample['input'],sample['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "printing_pairs=[evaluateFromDataset(random.randint(0,lenTrainSet-1)) for i in range(n_print)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTitle(best_model, input_tensor, max_len = 21):\n",
    "    best_model.eval()\n",
    "    n_words = wordbank.n_words\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        src_mask = best_model._generate_square_subsequent_mask(len(input_tensor))\n",
    "        src = best_model.pos_encoder(input_tensor*math.sqrt(best_model.ninp))\n",
    "        enc_output = best_model.transformer_encoder(src,src_mask)\n",
    "        \n",
    "        outputs = torch.zeros(max_len, dtype = torch.long)\n",
    "        outputs[0] = torch.LongTensor([SOS_token])\n",
    "#         print('enc:', enc_output)\n",
    "        for i in range(1, max_len):\n",
    "            trg_mask = best_model._generate_square_subsequent_mask(i)\n",
    "            trg = best_model.pos_encoder(best_model.decoder(outputs[:i].unsqueeze(1))*math.sqrt(best_model.ninp))\n",
    "#             print('trg:',trg)\n",
    "            dec_output = best_model.transformer_decoder(trg, enc_output, trg_mask)\n",
    "#             print('dec_out:', dec_output)\n",
    "            dec_output = best_model.out(dec_output)\n",
    "            out_flat = dec_output.view(-1,n_words)\n",
    "            final = F.log_softmax(out_flat, dim=1)\n",
    "\n",
    "            topv, topi = final[-1,:].data.topk(1)\n",
    "            outputs[i] = topi.item()\n",
    "            if topi.item() == EOS_token:\n",
    "                break\n",
    "                \n",
    "    return ' '.join(wordbank.index2word[idx] for idx in outputs[:i].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(best_model, n=10):\n",
    "    for i in range(n):\n",
    "        pair = printing_pairs[i]\n",
    "        print('input sequence: ', pair[0])\n",
    "        target_title = ' '.join(pair[1])\n",
    "        print('target title: ', target_title)\n",
    "        output_title = generateTitle(best_model, pair[2])\n",
    "        print('output title: ', output_title)\n",
    "        print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sequence:  ['Event Manager']\n",
      "target title:  Audio Engineer (Honorary)\n",
      "output title:  SOS\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Interpreter and Translator', 'Interpreter and Translator', 'F&B Attendant at Vantage Restaurant', 'Waiter and Bar Staff']\n",
      "target title:  Fashion Advisor\n",
      "output title:  SOS\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Technical Intern']\n",
      "target title:  Technical Intern\n",
      "output title:  SOS\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Senior Human Resource Executive']\n",
      "target title:  Country HR Specialist\n",
      "output title:  SOS\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Assistant Finance Manager', 'Finance Manager']\n",
      "target title:  Group Financial Controller\n",
      "output title:  SOS\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Receptionist', 'Special Education Teacher', 'Divison Lead, Silver Generation Office']\n",
      "target title:  Mental Health Coach\n",
      "output title:  SOS\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Sr Technician', 'engineer', 'Asst Manager']\n",
      "target title:  Production manager\n",
      "output title:  SOS\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Financial Accountant', 'Cost Accountant - Test and Assembly', 'Senior Financial Analyst, Valves Divison Asia Pacific', 'Finance Manager, Valves Division China Plant']\n",
      "target title:  Regional Finance Manager, Valves Division Asia Pacific\n",
      "output title:  SOS\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Finance & International Affair Executive']\n",
      "target title:  Territory Manager\n",
      "output title:  SOS\n",
      "--------------------------------------------------------------------------------\n",
      "input sequence:  ['Sales Engineer', 'Regional Sales Manager', 'General Manager', 'Business Development and Marketing Manager']\n",
      "target title:  Sales Manager China\n",
      "output title:  SOS\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 5, 1, 1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positionsDataset[3]['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
